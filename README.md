# ğŸ“¸ Image Caption Generator

Automatically generate captions for images using deep learning.  
This project combines a **CNN-based feature extractor** and an **LSTM-based language model** to describe images in natural language. The app is built with **Streamlit** for an interactive UI.  

---

## ğŸš€ Features
- Upload an image (`.jpg`, `.jpeg`, `.png`) and get an **AI-generated caption**.  
- Uses a **CNN encoder** (feature extractor) + **LSTM decoder** (caption generator).  
- Clean, modern **Streamlit UI** with sidebar, styled results, and loading animations.  
- Supports both `.keras` (Keras v3 format) and `.h5` (legacy HDF5 format) model files.  
- Pre-trained tokenizer for converting words â†” indices.  

---

## ğŸ“‚ Project Structure
```
.
â”œâ”€â”€ app.py                      # Streamlit app
â”œâ”€â”€ Image_Captioning.ipynb      # Training notebook
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ model.keras             # Trained caption model
â”‚   â”œâ”€â”€ feature_extractor.keras # CNN feature extractor
â”‚   â””â”€â”€ tokenizer.pkl           # Tokenizer used for captions
â””â”€â”€ README.md                   # Documentation
```

---

## âš™ï¸ Requirements
Install dependencies with:

```bash
pip install -r requirements.txt
```

**requirements.txt** (example):
```
streamlit
tensorflow
numpy
matplotlib
pillow
pickle5
```

---

## â–¶ï¸ Running the App

1. Clone the repository:
   ```bash
   git clone https://github.com/chandan9369/Image-Captioning-App-Using-NLP-and-Computer-Vision.git
   cd Image-Captioning-App-Using-NLP-and-Computer-Vision
   ```

2. Place your trained models inside the `models/` folder:
   - `model.keras` â†’ Caption generation model  
   - `feature_extractor.keras` â†’ CNN feature extractor  
   - `tokenizer.pkl` â†’ Tokenizer used during training  

3. Run the Streamlit app:
   ```bash
   streamlit run app.py
   ```

4. Open your browser at:  
   ğŸ‘‰ `http://localhost:8501`

---

## ğŸ§  Model Workflow
1. **Feature Extraction**  
   - Input image is resized to `(224x224)` and normalized.  
   - A CNN model (e.g., ResNet, VGG, Inception) extracts **feature embeddings**.  

2. **Sequence Generation**  
   - Extracted features + previously generated words are fed into an **LSTM**.  
   - The model predicts the **next word** until it reaches `"endseq"`.  

3. **Tokenizer**  
   - Maps words â†” integer indices.  
   - Ensures consistent vocabulary between training and inference.  

---

## ğŸ–¥ï¸ User Interface
- **Sidebar** â†’ Settings, app info, model description.  
- **Main Area** â†’  
  - Upload image  
  - Generate caption with **loading spinner**  
  - Display results in **two-column layout** (image + caption card).  

---

## ğŸ“˜ Example Usage

1. Upload an image of a **dog playing in the park**.  
2. The app may generate:  

   > *"a dog is playing on the grass"* ğŸ¶ğŸŒ³  

---

## ğŸ› ï¸ Training (Notebook: `Image_Captioning.ipynb`)
- Preprocess dataset (images + captions).  
- Train CNN feature extractor.  
- Train encoder-decoder (CNN + LSTM).  
- Save:
  - `model.keras`
  - `feature_extractor.keras`
  - `tokenizer.pkl`

---

## ğŸ“Œ Notes
- If you face issues loading `.keras` models in Streamlit Cloud, use:
  ```python
  load_model("model.keras", compile=False)
  ```
- For older environments, resave your model as `.h5`:
  ```python
  model.save("model.h5")
  ```

---

## ğŸ™Œ Acknowledgements
- TensorFlow / Keras team for deep learning frameworks.  
- Streamlit for the interactive web app.  
- Inspiration from various image captioning research papers.  
